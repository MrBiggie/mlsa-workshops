{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop RL02: Policy-Based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>It's a massive pain to get the gym environments to render in Jupyter notebooks, so **we're going to write the actual code into the \"policy_gradient.py\" file (in this repo)**<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Policy Gradient\n",
    "\n",
    "* http://karpathy.github.io/2016/05/31/rl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up OpenAI gym\n",
    "\n",
    "We're going to be using the environments from OpenAI's gym (https://gym.openai.com/) to try out Policy Gradient. You'll need to install a bunch of things to get this set up.\n",
    "\n",
    "Open up the Anaconda3 and install swig, gym, and box2d:\n",
    "\n",
    "    conda install -c anaconda swig\n",
    "    pip install --upgrade gym\n",
    "    pip install --upgrade box2d\n",
    "\n",
    "If you still get some kind of \"SWIG_CONSTANT\" error after doing all this, then try:  \n",
    "\n",
    "    pip3 install box2d box2d-kengz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Exercises\n",
    "1. Currently, our Agent has no policy function. Update `initialize_policy_network(self)` with the tensorflow code for initializing our policy network. Don't worry about the loss function or optimizer. *Hint 1:* input is state, output is a probability distribution over the action set. *Hint 2:* the dimensions of an input state are given to you as an instance variable (`state_shape`), as are the number of actions (`n_actions`). *Hint 3:* with our Agent implemented as a class, you need to make any of the tensorflow variables that you might need to access (i.e. input placeholders and any variables you call `run` on) instance variables.\n",
    "2. Fill out `choose_action(self, state)` to run the policy network. Generate a policy from the network, then sample an action (encoded as an integer) from it.\n",
    "3. Fill out `initialize_policy_network(self)` with the loss function and optimizer (read http://karpathy.github.io/2016/05/31/rl/) to find out exactly what these are going to be. Fill out `train(self)` to run the training loop for the network. *Hint:* the discounted rewards and one-hot actions arrays are provided for you in `train(self)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

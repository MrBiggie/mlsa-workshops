{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop RL02 - Policy-Based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>It's a massive pain to get the gym environments to render in Jupyter notebooks, so **we're going to write the actual code into the \"policy_gradient.py\" file (in this repo)**<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up OpenAI gym\n",
    "\n",
    "We're going to be using the environments from OpenAI's gym (https://gym.openai.com/) to try out Policy Gradient. You'll need to install a bunch of things to get this set up.\n",
    "\n",
    "Open up the Anaconda3 and install swig, gym, and box2d:\n",
    "\n",
    "    conda install -c anaconda swig\n",
    "    pip install --upgrade gym\n",
    "    pip install --upgrade box2d\n",
    "\n",
    "If you still get some kind of \"SWIG_CONSTANT\" error after doing all this, then try:  \n",
    "\n",
    "    pip3 install box2d box2d-kengz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-Based Reinforcement Learning\n",
    "\n",
    "* **Policy-Based** - reinforcement learning paradigm in which agents learn a *policy*.\n",
    "* **Policy** - a function that takes an environment state and outputs a probability distribution over all possible actions that can be taken. We construe the probabilities as measuring the relative value of each action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "* **Policy Gradient** - one of the algorithms that we can use to learn a policy.\n",
    "* **Deep Policy Gradient** - a variant of policy gradient in which we use a deep neural network as the policy function.\n",
    "\n",
    "### Deep Policy Gradient Algorithm\n",
    "1. Initialize the agent and environment\n",
    "2. Initialize the policy network with random weights\n",
    "3. Play game for an episode, storing transitions (s, a, r) for each timestep\n",
    "4. Update the policy network's weights\n",
    "5. Repeat 3 and 4 until network has learned a decent policy\n",
    "\n",
    "### The Policy Network\n",
    "\n",
    "* **For generating policies**, the network must take *current state as input* and generate a *probability distribution over the set of actions as output*\n",
    "* **For training**, the network will need \n",
    "\n",
    "#### Architecture\n",
    "\n",
    "* Input: the game state (and some other things)\n",
    "* Output: a probability distribution over all possible actions\n",
    "\n",
    "#### Loss Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

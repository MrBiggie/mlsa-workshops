{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop PR02: Intro to modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "* Practice `pandas` for manipulating data\n",
    "* Cursory understanding of using `sklearn` for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping the data\n",
    "This is the part we'll leave up to you:\n",
    "\n",
    "* Read in the titanic data\n",
    "* Read the training labels (\"Survived\") into a variable called `train_y` (calling the labels \"y\" is a convention)\n",
    "* Generate a new feature \"SexEncoded\", which encodes the column \"Sex\" as 0 if \"female\" and 1 if \"male\". This is necessary because most of our classifiers need to be fed numbers (they can't understand raw strings)\n",
    "* Read \"Age\", \"SexEncoded\", \"Pclass\", and \"Fare\" into a variable called `train_X` (also a conventional name)\n",
    "\n",
    "Go back to Workshop PR01 if you need to double check how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "df = pd.read_csv(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training labels into train_y\n",
    "train_y = df[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the new feature \"SexEncoded\"\n",
    "df[\"SexEncoded\"] = df[\"Sex\"].apply(lambda sex: 0 if \"female\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training features into train_X\n",
    "train_X = df[[\"Age\", \"SexEncoded\", \"Pclass\", \"Fare\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with `sklearn`\n",
    "\n",
    "`sklearn` makes it super easy to build models - it comes with a fat mixed bag of pre-implemented models (https://scikit-learn.org/stable/supervised_learning.html). But the sequence of steps you need to go through to run them is always the same:\n",
    "1. Import the classifier from sklearn (nobody every imports the whole `sklearn` library, it's just too large)\n",
    "2. Instantiate a classifier object\n",
    "3. Train the classifier object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) let's import sklearn's RandomForestClassifier\n",
    "# side note: as far as algorithms go, the random forest algorithm is actually pretty straightforward\n",
    "#   you an look it up here (https://en.wikipedia.org/wiki/Random_forest#Algorithm), but you might have\n",
    "#   to do a bit of detour reading on decision trees and bootstrap aggregating\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) instantiate a RandomForestClassifier object\n",
    "random_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3) and train using \"train_X\" and \"train_y\"\n",
    "random_forest.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9447576099210823"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and if you want to see how it performs\n",
    "random_forest.score(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually, it's a bit dodgy for us to be scoring our classifier using the same data that we used to train it\n",
    "# what we should really be doing is setting aside some portion of the data to be used as \"test\" data, not using\n",
    "# that portion in training, and then using it for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
